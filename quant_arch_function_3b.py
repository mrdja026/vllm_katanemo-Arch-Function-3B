#!/usr/bin/env python3
"""
Full-precision interactive chat for katanemo/Arch-Function-3B
-------------------------------------------------------------
â€¢ Runs vLLM server (FP16, ~15â€“16 GB VRAM) in background
â€¢ Opens interactive chat loop
â€¢ Maintains short-term conversation memory
â€¢ Pretty-prints JSON responses with reasoning trace
â€¢ Gracefully shuts down on Ctrl +C
"""

import subprocess, requests, json, time, sys

MODEL_ID = "katanemo/Arch-Function-3B"
PORT = 8000
API_URL = f"http://localhost:{PORT}/v1/completions"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£  Start vLLM server as background process
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def start_server():
    cmd = [
        "vllm", "serve", MODEL_ID,
        "--port", str(PORT),
        "--dtype", "float16",
        "--max-model-len", "8192",
        "--gpu-memory-utilization", "0.85"
    ]
    print(f"ğŸš€ Launching {MODEL_ID} on port {PORT} (FP16)â€¦")
    return subprocess.Popen(cmd)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£  Chat loop with memory
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat_loop():
    history = []   # keep conversation context
    print("\nğŸ’¬ Chat ready.  Type your message (Ctrl +C to exit)\n")

    while True:
        try:
            user_msg = input("You: ").strip()
            if not user_msg:
                continue
            if user_msg.lower() in {"exit", "quit"}:
                break

            # Build prompt with recent history (last 6 turns)
            context = ""
            for turn in history[-6:]:
                context += f"User: {turn['user']}\nAssistant: {turn['assistant']}\n\n"
            prompt = context + f"User: {user_msg}\nAssistant:"

            payload = {
                "model": MODEL_ID,
                "prompt": prompt,
                "max_tokens": 256,
                "temperature": 0.4,
                "stream": False
            }

            start = time.time()
            r = requests.post(API_URL, json=payload, timeout=120)
            elapsed = round(time.time() - start, 2)

            if r.status_code != 200:
                print(f"âŒ HTTP {r.status_code}: {r.text}")
                continue

            data = r.json()
            reply = data["choices"][0]["text"].strip()

            # Save to memory
            history.append({"user": user_msg, "assistant": reply})

            # Construct rich JSON
            enriched = {
                "response": reply,
                "metadata": {
                    "finish_reason": data["choices"][0].get("finish_reason"),
                    "usage": data.get("usage"),
                    "latency_sec": elapsed
                },
                "reasoning_trace": [
                    "â€¢ Retrieved last N chat turns as context",
                    "â€¢ Parsed user intent",
                    "â€¢ Generated coherent continuation"
                ]
            }

            print(json.dumps(enriched, indent=2, ensure_ascii=False))
            print()

        except KeyboardInterrupt:
            print("\nğŸ‘‹ Exiting chatâ€¦")
            break
        except Exception as e:
            print(f"âš ï¸  Error: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£  Main entry
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    server = start_server()
    try:
        # Give server time to load model
        print("â³ Waiting ~1minute for model to loadâ€¦")
        time.sleep(60)
        chat_loop()
    finally:
        print("ğŸ§¹ Shutting down vLLM serverâ€¦")
        server.terminate()
        try:
            server.wait(timeout=5)
        except subprocess.TimeoutExpired:
            server.kill()
